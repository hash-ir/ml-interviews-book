{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions of the Machine Learning Interviews book by Chip Huyen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports used throughout the notebook\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Algebra and (little) calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. The dot product of two vectors determines how the vectors are aligned w.r.t each other. Higher the dot product, more the two vectors will be aligned\n",
    "\n",
    "ii The dot product is maximum implies $u.v = ||u|| ||v||cos\\theta = ||u||$. Therefore, $v$ will just be a unit vector in the direction of $u$ i.e $v = \\frac{u}{||u||}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Outer product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i $a = np.array([3, 2, 1]), b = np.array([-1, 0, 1])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Two vectors a & b are linearly independent if a =/= cb where c is a constant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. The span dimennsion is d since each vector can be represented by a basis vector of dimension d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 \n",
    "i. Norm determines the length or magnitude of vectors in a space.\n",
    "- $L_0 = \\sum_{i=1}^{n}|x_i|^0 = \\{\\#i:x_i \\ne 0\\}$ s.t $0^0 = 0$ It is the number of non-zero elements in the vector. \n",
    "- $L_1 = \\sum_{i=1}^{n}|x_i|$\n",
    "- $L_2 = \\sqrt{\\sum_{i=1}^{n}|x_i|^2}$\n",
    "- $L_{norm} = L_p = (\\sum_{i=1}^{n}|x_i|^p)^\\frac{1}{p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Because there exists vectors x s.t Mx = lx where l is a constant known as the eigenvalue. \n",
    "# In this case, the matrix multiplication is just a linear transformation of the vector x by the value l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. The inverse of a matrix M reverses the transformation done by the matrix M. \n",
    "# In other words, applying the matrix inverse following the matrix transformation is \n",
    "# equivalent to applying no transformation at all. This is mathematically justified by MM^-1 = I\n",
    "# where I is the identity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. tr(A) = 3+3+2-1 = 7, det(A) = 3*3*2*-1 = -18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. \n",
    "# i\n",
    "# x = A^(-1)b\n",
    "\n",
    "# ii\n",
    "# when A^(-1) exists\n",
    "\n",
    "# iii\n",
    "# \n",
    "\n",
    "# iv\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Derivative\n",
    "# i\n",
    "# The derivative represents the change in the output corresponding to the change in the input\n",
    "\n",
    "# ii\n",
    "# Derivative works for scalars, gradient works for 1d vectors and Jacobian works for matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. J in R^(mxd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing\n",
      "Error parsing\n",
      "Error parsing\n",
      "Error parsing\n",
      "Error parsing\n",
      "Error parsing\n",
      "Error parsing\n",
      "Error parsing\n",
      "Error parsing\n",
      "Error parsing\n",
      "Error parsing\n",
      "Error parsing\n",
      "Error parsing\n",
      "Error parsing\n",
      "Error parsing\n",
      "Error parsing\n",
      "Error parsing\n",
      "Error parsing\n",
      "Error parsing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '0',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '4',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '4',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '3',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '2',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '1',\n",
       " '']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 10. Calculating a mathematical expression\n",
    "inp = \"10 * 4 + (4 + 3) / (2 - 1)\"\n",
    "\n",
    "stack = []\n",
    "def check(inp):\n",
    "    num = \"\"\n",
    "    for c in inp:\n",
    "        try:\n",
    "            int(c)\n",
    "            num += c\n",
    "        except:\n",
    "            print(\"Error parsing\")\n",
    "            pass\n",
    "        finally:\n",
    "            stack.append(num)\n",
    "            num = \"\"\n",
    "            \n",
    "check(inp)\n",
    "stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Sampling and creating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. 6C2 + 4C1 = 15 + 4 = 19 ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Sampling with replacement leads to unique samples, without replacement may have duplicate samples\n",
    "# We should use w/ replacement when we want to randomly sample indices or locations of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Objective functions, metrics, and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 \n",
    "The problem in this case is that the data will have a huge class imbalance. I would know that the relative proportion of people having cancer will be very less. Let's consider that there were 1000 patients in total. And out of these, only 1 patient had cancer. Then the accuracy in this case will come out to be 99.9% if the model correctly predicted the patients who did not have cancer. In other words, $TP = 0, FP = 0, TN = 999, FN = 1$ and: \n",
    "\n",
    "$$Acc = \\frac{TP + TN}{TP + FP + TN + FN} = \\frac{0+999}{0+0+999+1} = 99.9\\%$$\n",
    "\n",
    "But the precison and recall in this case will be 0 since the model did not predict the patient with cancer. Hence, the F1-score, in this case, will also be zero. We care about the precision and recall because the patients having cancer but predicted incorrectly is more dangerous than predicting if the patient has cancer but is actually healthy. To summarize, whenever there is a class imbalance or the False Positives and False Negatives are more important, the accuracy is a bad metric and F1-score should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7\n",
    "i. F1 score is a better metric when there are imbalanced classes in the dataset (as seen in the previous question). Also, when it more important to look at the False Positives and False Negatives, F1-score should be preferred instead of accuracy.\n",
    "\n",
    "ii. Yes, we can use F1-score for multi-class classification problems. There are three variants-macro-averaging, micro-averaging and weighted-averaging. \n",
    "- In macro-averaging, the per-class precision, recall and F1-scores are computed. Then the macro-averaged F1-score is the mean of the per-class scores.\n",
    "- In micro-averaging, first the micro-averaged precison and recall are computed. In this case, all the samples are considered together. This leads to equal values of the precision and recall and since $F1 = \\frac{2*P*R}{P + R}$, F1-score is equal to the precision and recall.\n",
    "- In weighted-averaging,  the per-class F1-scores are weighted by the class support and then averaged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8\n",
    "i. $$P = \\frac{30}{30+5} = 0.86$$ \n",
    "$$R = \\frac{30}{30+20} = 0.50$$\n",
    "$$F1 = \\frac{2*0.86*0.50}{0.86+0.50} = 0.63$$\n",
    "\n",
    "ii. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16\n",
    "i. MPE or MLE (Maximum Likelihood Estimation) finds the parameters that optimize the likelihood of the data whereas MAP finds the optimal parameters after seeing the data. In MLE, we care about the data distribution $p(D|\\theta)$ while in MAP, we care about the posterior distribution $p(\\theta|D)$.\n",
    "\n",
    "ii. Let's consider the example of coin flips. Given a sequence of coin flips, we have to predict the next flip. Let $\\theta$ denote the probability of getting a tail in the next flip i.e $p(F=\\mathcal{T}) = \\theta$. For MLE, we have the point estimate $\\theta_{MPE} = \\frac{\\mathcal{T}}{\\mathcal{T} + \\mathcal{H}}$. And for MAP, if we choose Beta as the prior distribution with $a=2, b=1$, we have the point estimate $\\theta_{MAP} = \\frac{\\mathcal{T}+a-1}{\\mathcal{T}+\\mathcal{H}+a+b-2}$. Beta is the distribution which approximates other distributions based on the parameters $a$ and $b$. Beta is also the conjugate prior of Bernoulli distribution which allows for a closed form solution of the MAP. \n",
    "\n",
    "Now, if we have the following sequence of coin flips:\n",
    "$$\\mathcal{H} \\space\\space \\mathcal{H}$$\n",
    "- $\\theta_{MPE} = 0$ since there were no tails in the previous flips \n",
    "- $\\theta_{MAP} = 0.33$\n",
    "\n",
    "Hence, MPE and MAP would produce different results when the prior distribution is not uniform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.2 Classical Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2\n",
    "Feature scaling is important where the data is not linearly separable. Since Logistic Regression is a linear classifier, it will not be able to separate the classes in the dataset if feature scaling is not performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3\n",
    "Fraud detection is a binary classification problem. The algorithms to use for this are Logistic Regression, SVM, Random Forest, MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6\n",
    "i. k is a hyperparameter. Its value should be chosen based on the cross-validation trials which lead to the maximum accuracy.\n",
    "\n",
    "ii. When the value is increased, the accuracy increases but after a certain value, the accuracy will decrease.\n",
    "\n",
    "iii. When k is small, there is more bias and less variance. As k increases, bias decreases and variance increases. For large k, bias is small and variance is high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9\n",
    "i. $$A_{directed} = \n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 0 & 1 & 1 \\\\\n",
    "0 & 0 & 1 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "ii. $$A_{undirected} = \n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 0 & 1 & 1 \\\\\n",
    "1 & 0 & 1 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 0 \\\\\n",
    "1 & 1 & 0 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "iii. The adjacency matrices of two isomorphic graphs will be equal upto some ordering of the vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.4 Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1\n",
    "Autoencoders are useful for dimensionality reduction or learning a latent representation of data. They are particularly useful in the case of unsupervised learning where there is no labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.3 Training neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.33706809,  0.70988615, -0.59967289]),\n",
       " array([ 0.20920099, -0.28525913]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([3, 4])\n",
    "y = 2\n",
    "w1 = np.random.randn(len(x)+1)\n",
    "w2 = np.random.randn(len(x))\n",
    "\n",
    "def relu(x):\n",
    "    return x if x>0 else 0\n",
    "\n",
    "w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "def forward(x, w1, w2):\n",
    "    h = w1.dot(np.concatenate(([1], x)))\n",
    "    h_relu = relu(h)\n",
    "    # print(h_relu)\n",
    "    y_hat = w2.dot(np.concatenate(([1.0], [h_relu])))\n",
    "    cache = tuple([x, h_relu])\n",
    "    \n",
    "    return y_hat, cache\n",
    "\n",
    "def backward(loss, y_hat, cache):\n",
    "    x, h_relu = cache\n",
    "    dloss = -2*(y-y_hat)\n",
    "    dy_hat = w2[1]\n",
    "    dh_relu = 1 if h_relu > 0 else 0\n",
    "    dh = np.concatenate(([1], x))\n",
    "    dw1 = dloss*dy_hat*dh_relu*dh\n",
    "    dw2 = dloss*h_relu\n",
    "    \n",
    "    return dw1, dw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1981451676147725\n",
      "3.1981451676147725\n",
      "3.1981451676147725\n",
      "3.1981451676147725\n",
      "3.1981451676147725\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "for i in range(5):\n",
    "    y_hat, cache = forward(x, w1, w2)\n",
    "    loss = (y-y_hat)**2\n",
    "    dw1, dw2 = backward(loss, y_hat, cache)\n",
    "    update_w1 = -lr*dw1\n",
    "    update_w2 = -lr*dw2\n",
    "    w1 += update_w1\n",
    "    w2 += update_w2\n",
    "    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.32674053,  0.67890347, -0.64098313]),\n",
       " array([ 0.21166413, -0.28279598]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.21166413456119135, 2)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(x, w1, w2)[0], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
